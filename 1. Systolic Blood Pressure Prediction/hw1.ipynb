{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Te27fi-0pP"
      },
      "source": [
        "# **HW1: Regression**\n",
        "In *assignment 1*, you need to finish:\n",
        "\n",
        "1.  Basic Part: Implement two regression models to predict the Systolic blood pressure (SBP) of a patient. You will need to implement **both Matrix Inversion and Gradient Descent**.\n",
        "\n",
        "\n",
        "> *   Step 1: Split Data\n",
        "> *   Step 2: Preprocess Data\n",
        "> *   Step 3: Implement Regression\n",
        "> *   Step 4: Make Prediction\n",
        "> *   Step 5: Train Model and Generate Result\n",
        "\n",
        "2.  Advanced Part: Implement one regression model to predict the SBP of multiple patients in a different way than the basic part. You can choose **either** of the two methods for this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDdnos-4uUv"
      },
      "source": [
        "# **1. Basic Part (55%)**\n",
        "In the first part, you need to implement the regression to predict SBP from the given DBP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_EVqWlB-DTF"
      },
      "source": [
        "## 1.1 Matrix Inversion Method (25%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_mi.csv**\n",
        "*   Print your coefficient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCR7vk9BFkf"
      },
      "source": [
        "### *Import Packages*\n",
        "\n",
        "> Note: You **cannot** import any other package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HL5XjqFf4wSj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C58G-kmsLiqV",
        "outputId": "e9095a53-6e14-43fb-d9cb-821b24d29610"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnWjrzi0dMPz"
      },
      "source": [
        "### *Global attributes*\n",
        "Define the global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EWLDPOlHBbcK"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_basic_mi.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsFC-cvqIcYK"
      },
      "source": [
        "You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8duZN2LGg1Cc"
      },
      "outputs": [],
      "source": [
        "def Mape(actual, predicted):\n",
        "  ape = np.abs((actual - predicted) / np.maximum(np.abs(actual), 1e-7)) * 100\n",
        "  mape = np.mean(ape)\n",
        "  return round(mape, 4)\n",
        "\n",
        "training_dataset = [] # dataset for DBP, i.e. features\n",
        "validation_dataset = [] # dataset for SBP, i.e. labels\n",
        "testing_dataset = []\n",
        "weights = [] # computed weights from matrix inversion\n",
        "\n",
        "training_mode = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoRFoQjBW5S"
      },
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "dekR1KnqBtI6"
      },
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYPuikLCFx4"
      },
      "source": [
        "### *Implement the Regression Model*\n",
        "\n",
        "> Note: It is recommended to use the functions we defined, you can also define your own functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWwdx06JNEYs"
      },
      "source": [
        "#### Step 1: Split Data\n",
        "Split data in *training_datalist* into training dataset and validation dataset\n",
        "* Validation dataset is used to validate your own model without the testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "USDciENcB-5F"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "  _training = []\n",
        "  _validation = []\n",
        "  _testing = []\n",
        "\n",
        "  for training_data in training_datalist[1:]:\n",
        "    _training.append(int(training_data[0]))\n",
        "    _validation.append(int(training_data[1]))\n",
        "\n",
        "  for testing_data in testing_datalist[1:]:\n",
        "    _testing.append(int(testing_data[0]))\n",
        "\n",
        "  return np.array(_training), np.array(_validation), np.array(_testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-3Qln4aNgVy"
      },
      "source": [
        "#### Step 2: Preprocess Data\n",
        "Handle the unreasonable data\n",
        "> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "XXvW1n_5NkQ5"
      },
      "outputs": [],
      "source": [
        "def PreprocessData():\n",
        "  # dbp: 60~80, sbp: 90~120\n",
        "  num_data = training_dataset.size\n",
        "  for i, dbp, sbp in zip(range(num_data), training_dataset, validation_dataset):\n",
        "    if dbp is not int:\n",
        "      training_dataset[i] = dbp = int(dbp)\n",
        "    if sbp is not int:\n",
        "      validation_dataset[i] = sbp = int(sbp)\n",
        "    if dbp < 20 or dbp > 240:\n",
        "      training_dataset[i] = 70\n",
        "    if sbp < 30 or sbp > 360:\n",
        "      validation_dataset[i] = 105"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDLpJmQUN3V6"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Matrix Inversion to finish this part\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Tx9n1_23N8C0"
      },
      "outputs": [],
      "source": [
        "def MatrixInversion():\n",
        "  global weights\n",
        "  # add bias term in training dataset\n",
        "  training_bias = np.vstack((np.ones_like(training_dataset), training_dataset)).T\n",
        "  # compute inverse matrix of bias\n",
        "  training_bias_inverse = np.linalg.pinv(training_bias)\n",
        "  # compute weights by using inverse bias\n",
        "  weights = training_bias_inverse.dot(validation_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NxRNFwyN8xd"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "Make prediction of testing dataset and store the value in *output_datalist*\n",
        "The final *output_datalist* should look something like this\n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "EKlDIC2-N_lk"
      },
      "outputs": [],
      "source": [
        "def MakePrediction(testing_dataset):\n",
        "  testing_bias = np.vstack((np.ones_like(testing_dataset), testing_dataset)).T\n",
        "  predictions = testing_bias.dot(weights)\n",
        "  predictions = (np.round(predictions, decimals=4)).astype(float)\n",
        "\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCd0Z6izOCwq"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCL92EPKOFIn",
        "outputId": "d84306e0-e9c9-4856-f2b9-6732aec9319f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47.49053479094964 0.9927147813478133\n",
            "Validations Mape: 5.5434\n"
          ]
        }
      ],
      "source": [
        "training_dataset, validation_dataset, testing_dataset = SplitData()\n",
        "PreprocessData()\n",
        "MatrixInversion() # compute weights\n",
        "print(*weights)\n",
        "output_datalist = MakePrediction(testing_dataset)\n",
        "\n",
        "if training_mode:\n",
        "  validations = MakePrediction(training_dataset)\n",
        "  mape = Mape(validation_dataset, validations)\n",
        "  print(f'Validations Mape: {mape}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Jhd8wAOk3D"
      },
      "source": [
        "### *Write the Output File*\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "tYQVYLlKOtDB"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow([row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3WOhglA9ML"
      },
      "source": [
        "## 1.2 Gradient Descent Method (30%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_gd.csv**\n",
        "*   Output your coefficient update in a csv file **hw1_basic_coefficient.csv**\n",
        "*   Print your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkMqa_xjXhEv"
      },
      "source": [
        "### *Global attributes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "wNZtRWUeXpEu"
      },
      "outputs": [],
      "source": [
        "output_dataroot = 'hw1_basic_gd.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "coefficient_output_dataroot = 'hw1_basic_coefficient.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']\n",
        "\n",
        "coefficient_output = [] # Your coefficient update during gradient descent\n",
        "                   # Should be a (number of iterations * number_of coefficient) matrix\n",
        "                   # The format of each row should be ['w0', 'w1', ...., 'wn']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5DeHxdLdai3"
      },
      "source": [
        "Your own global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "_2IO5tYSdaFd"
      },
      "outputs": [],
      "source": [
        "# computed weights and bias from gradient descent\n",
        "trained_weights = []\n",
        "trained_bias = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVBLT1aqXuW0"
      },
      "source": [
        "### *Implement the Regression Model*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPWpcOnXhCZ"
      },
      "source": [
        "#### Step 1: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1PEf_qGvYHu0"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "  _training = []\n",
        "  _validation = []\n",
        "  _testing = []\n",
        "\n",
        "  for training_data in training_datalist[1:]:\n",
        "    _training.append(int(training_data[0]))\n",
        "    _validation.append(int(training_data[1]))\n",
        "\n",
        "  for testing_data in testing_datalist[1:]:\n",
        "    _testing.append(int(testing_data[0]))\n",
        "\n",
        "  return np.array(_training), np.array(_validation), np.array(_testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpSoPDPKX56w"
      },
      "source": [
        "#### Step 2: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "uLTXOWRwYHiS"
      },
      "outputs": [],
      "source": [
        "def PreprocessData():\n",
        "  global training_dataset, testing_dataset\n",
        "  num_data = training_dataset.size\n",
        "  for i, dbp, sbp in zip(range(num_data), training_dataset, validation_dataset):\n",
        "    if dbp is not int:\n",
        "      training_dataset[i] = dbp = int(dbp)\n",
        "    if sbp is not int:\n",
        "      validation_dataset[i] = sbp = int(sbp)\n",
        "    if dbp < 20 or dbp > 240:\n",
        "      training_dataset[i] = 70\n",
        "    if sbp < 30 or sbp > 360:\n",
        "      validation_dataset[i] = 105\n",
        "\n",
        "  training_dataset = training_dataset.reshape(-1, 1)\n",
        "  testing_dataset = testing_dataset.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_y82gXX6a-"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Gradient Descent to finish this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "-635Ee00YHTE"
      },
      "outputs": [],
      "source": [
        "def GradientDescent(learning_rate, num_iterations):\n",
        "  num_samples, num_features = 373, 1\n",
        "  weights = np.zeros((1,))\n",
        "  bias = 0\n",
        "  coefficients = []\n",
        "\n",
        "  for i in range(num_iterations):\n",
        "    predictions = np.dot(training_dataset, weights) + bias\n",
        "\n",
        "    errors = predictions - validation_dataset\n",
        "\n",
        "    gradient_weights = (1 / num_samples) * np.dot(training_dataset.T, errors)\n",
        "    gradient_bias = (1 / num_samples) * np.sum(errors)\n",
        "\n",
        "    weights -= learning_rate * gradient_weights\n",
        "    bias -= learning_rate * gradient_bias\n",
        "\n",
        "    # store the coefficients while updating\n",
        "    # weights is (1, 1) matrix, so add [0] to trans the array to number form\n",
        "    coefficients.append(weights[0])\n",
        "\n",
        "  return weights, bias, coefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLuPxs2ZX21S"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "\n",
        "Make prediction of testing dataset and store the values in *output_datalist*\n",
        "The final *output_datalist* should look something like this\n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP\n",
        "\n",
        "Remember to also store your coefficient update in *coefficient_output*\n",
        "The final *coefficient_output* should look something like this\n",
        "> [ [1, 0, 3, 5], ... , [0.1, 0.3, 0.2, 0.5] ] where each row contains the [w0, w1, ..., wn] of your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "8pnNDlQeYGtE"
      },
      "outputs": [],
      "source": [
        "def MakePrediction(testing_dataset):\n",
        "  predictions = np.dot(testing_dataset, trained_weights) + trained_bias\n",
        "  predictions = (np.round(predictions, decimals=4)).astype(float)\n",
        "\n",
        "  if training_mode:\n",
        "    print(f'dataset shape: {testing_dataset.shape}')\n",
        "    print(f'weights shape: {trained_weights.shape}')\n",
        "    print(f'predictions shape: {predictions.shape}')\n",
        "\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IScbxxMAYAgZ"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90EisOc7YG-N",
        "outputId": "ebdbf93c-1fac-4586-e83a-b6f243f6775e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.4964836443931884\n",
            "dataset shape: (20, 1)\n",
            "weights shape: (1,)\n",
            "predictions shape: (20,)\n",
            "dataset shape: (373, 1)\n",
            "weights shape: (1,)\n",
            "predictions shape: (373,)\n",
            "Validations Mape: 6.859\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.0001\n",
        "num_iterations = 50000\n",
        "\n",
        "PreprocessData()\n",
        "trained_weights, trained_bias, coefficient_output = GradientDescent(learning_rate, num_iterations)\n",
        "print(*trained_weights)\n",
        "output_datalist = MakePrediction(testing_dataset)\n",
        "\n",
        "if training_mode:\n",
        "  validations = MakePrediction(training_dataset)\n",
        "  mape = Mape(validation_dataset, validations)\n",
        "  print(f'Validations Mape: {mape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1DpV_HcYFpl"
      },
      "source": [
        "### *Write the Output File*\n",
        "\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "**Write the coefficient update to csv**\n",
        "> Format: 'w0', 'w1', ..., 'wn'\n",
        ">*   The number of columns is based on your number of coefficient\n",
        ">*   The number of row is based on your number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "NLSHgpDvDXNI"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow([row])\n",
        "\n",
        "with open(coefficient_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in coefficient_output:\n",
        "    writer.writerow([row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4408qg4xMQ"
      },
      "source": [
        "# **2. Advanced Part (40%)**\n",
        "In the second part, you need to implement the regression in a different way than the basic part to help your predictions of multiple patients SBP.\n",
        "\n",
        "You can choose **either** Matrix Inversion or Gradient Descent method.\n",
        "\n",
        "The training data will be in **hw1_advanced_training.csv** and the testing data will be in **hw1_advanced_testing.csv**.\n",
        "\n",
        "Output your prediction in **hw1_advanced.csv**\n",
        "\n",
        "Notice:\n",
        "> You cannot import any other package other than those given\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDWpcGhDc18p"
      },
      "source": [
        "### Input the training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "v66HUClZcxaE"
      },
      "outputs": [],
      "source": [
        "training_dataroot_adv = 'hw1_advanced_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot_adv = 'hw1_advanced_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot_adv = 'hw1_advanced.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist_adv =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist_adv =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist_adv =  [] # Your prediction, should be 220 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMyfoG93c18q"
      },
      "source": [
        "### Your Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "UsqI0mntc18q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "testing dataset shape: (220, 4)\n",
            "weights shape: (4,)\n",
            "predictions shape: (220,)\n",
            "testing dataset shape: (5696, 4)\n",
            "weights shape: (4,)\n",
            "predictions shape: (5696,)\n",
            "Validations Mape: 13.5698\n"
          ]
        }
      ],
      "source": [
        "training_dataroot_adv = 'hw1_advanced_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot_adv = 'hw1_advanced_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot_adv = 'hw1_advanced.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist_adv =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist_adv =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist_adv =  [] # Your prediction, should be 220 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']\n",
        "\n",
        "# global attributes\n",
        "training_dataset_adv = [] # dataset for all features\n",
        "\n",
        "validation_dataset_adv = [] # dataset for known labels\n",
        "testing_dataset_adv = []\n",
        "trained_weights_adv = []\n",
        "trained_bias_adv = 0\n",
        "\n",
        "# Read input csv to datalist\n",
        "with open(training_dataroot_adv, newline='') as csvfile:\n",
        "  training_datalist_adv = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot_adv, newline='') as csvfile:\n",
        "  testing_datalist_adv = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "\n",
        "# split data\n",
        "def SplitData_adv():\n",
        "  _training = []\n",
        "  _validation = []\n",
        "  _testing = []\n",
        "\n",
        "  for training_data in training_datalist_adv[1:]:\n",
        "    _training.append(training_data[2:6])\n",
        "    _validation.append(int(training_data[6]))\n",
        "\n",
        "  for testing_data in testing_datalist_adv[1:]:\n",
        "    _testing.append(testing_data[2:6])\n",
        "\n",
        "  return np.array(_training), np.array(_validation), np.array(_testing)\n",
        "\n",
        "# preprocess data\n",
        "def PreprocessData_adv():\n",
        "  global training_dataset_adv, validation_dataset_adv, testing_dataset_adv\n",
        "\n",
        "  num_samples, num_features = training_dataset_adv.shape\n",
        "  for i in range(num_samples):\n",
        "    for j in range(num_features):\n",
        "      if training_dataset_adv[i][j] == '':\n",
        "        training_dataset_adv[i][j] = -1\n",
        "\n",
        "  training_dataset_adv = training_dataset_adv.astype(float)\n",
        "  validation_dataset_adv = validation_dataset_adv.astype(float)\n",
        "  testing_dataset_adv = testing_dataset_adv.astype(float)\n",
        "\n",
        "  for i in range(num_samples):\n",
        "    # temperature\n",
        "    if training_dataset_adv[i][0] < 80 or training_dataset_adv[i][0] > 130:\n",
        "      training_dataset_adv[i][0] = 98\n",
        "    # heartrate\n",
        "    if training_dataset_adv[i][1] < 40 or training_dataset_adv[i][1] > 180:\n",
        "      training_dataset_adv[i][1] = 80\n",
        "    # resprate\n",
        "    if training_dataset_adv[i][2] < 5 or training_dataset_adv[i][2] > 40:\n",
        "      training_dataset_adv[i][2] = 16\n",
        "    # o2sat\n",
        "    if training_dataset_adv[i][3] < 70 or training_dataset_adv[i][3] > 100:\n",
        "      training_dataset_adv[i][3] = 95\n",
        "\n",
        "    if validation_dataset_adv[i] < 30 or validation_dataset_adv[i] > 360:\n",
        "      validation_dataset_adv[i] = 105\n",
        "\n",
        "  training_dataset_adv = training_dataset_adv.reshape(-1, 4)\n",
        "  testing_dataset_adv = testing_dataset_adv.reshape(-1, 4)\n",
        "\n",
        "\n",
        "# implement regression\n",
        "def GradientDescent_adv(learning_rate_adv, num_iterations):\n",
        "  num_samples, num_features = training_dataset_adv.shape\n",
        "  weights = np.zeros((num_features,))\n",
        "  bias = 0\n",
        "\n",
        "  for _ in range(num_iterations):\n",
        "    predictions = np.dot(training_dataset_adv, weights) + bias\n",
        "\n",
        "    errors = predictions - validation_dataset_adv\n",
        "\n",
        "    gradient_weights = (1 / num_samples) * np.dot(training_dataset_adv.T, errors)\n",
        "    gradient_bias = (1 / num_samples) * np.sum(errors)\n",
        "\n",
        "    weights -= learning_rate_adv * gradient_weights\n",
        "    bias -= learning_rate_adv * gradient_bias\n",
        "\n",
        "  return weights, bias\n",
        "\n",
        "# make predictions\n",
        "def MakePrediction_adv(testing_dataset_adv):\n",
        "  predictions = np.dot(testing_dataset_adv, trained_weights_adv) + trained_bias_adv\n",
        "  predictions = (np.round(predictions, decimals=4)).astype(float)\n",
        "\n",
        "  if training_mode:\n",
        "    print(f'testing dataset shape: {testing_dataset_adv.shape}')\n",
        "    print(f'weights shape: {trained_weights_adv.shape}')\n",
        "    print(f'predictions shape: {predictions.shape}')\n",
        "\n",
        "  return predictions\n",
        "\n",
        "\n",
        "# train model and generate results\n",
        "learning_rate_adv = 0.00001\n",
        "num_iteration_adv = 100000\n",
        "\n",
        "training_dataset_adv, validation_dataset_adv, testing_dataset_adv = SplitData_adv()\n",
        "PreprocessData_adv()\n",
        "trained_weights_adv, trained_bias_adv = GradientDescent_adv(learning_rate_adv, num_iteration_adv)\n",
        "output_datalist_adv = MakePrediction_adv(testing_dataset_adv)\n",
        "\n",
        "if training_mode:\n",
        "  validations = MakePrediction_adv(training_dataset_adv)\n",
        "  mape = Mape(validation_dataset_adv, validations)\n",
        "  print(f'Validations Mape: {mape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "485R1vbNc18r"
      },
      "source": [
        "### Output your Prediction\n",
        "\n",
        "> your filename should be **hw1_advanced.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "UOSpBTmcc18r"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow([row])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtgCJU7FPeJL"
      },
      "source": [
        "# Report *(5%)*\n",
        "\n",
        "Report should be submitted as a pdf file **hw1_report.pdf**\n",
        "\n",
        "*   Briefly describe the difficulty you encountered\n",
        "*   Summarize your work and your reflections\n",
        "*   No more than one page\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlEE53_MPf4W"
      },
      "source": [
        "# Save the Code File\n",
        "Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
